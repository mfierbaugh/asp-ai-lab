{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfierbaugh/asp-ai-lab/blob/main/openai_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHLUjwLEz6dL"
      },
      "source": [
        "# OpenAI LAB - Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7kHzEovgXKJ"
      },
      "source": [
        "This lab combines the previous two labs and will show how we can retrieve information locally and augment the input.  Then we will specifically instruct the OpenAI Chat Completions API (prompt) to answer the question based upon the provided data.  This is an example of RAG (Retrieval Augmented Generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TF5u2vp0HSQ"
      },
      "source": [
        "## Setting up the runtime enviornment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h02rE0iq0UEw"
      },
      "source": [
        "Clone the github so that we have the files we need in the local runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiaspeClgP7U"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mfierbaugh/asp-ai-lab.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGe_3tFkhLTX"
      },
      "source": [
        "Install the required python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xer4YEB4hX5X",
        "outputId": "e0a6684f-2a98-46ee-d46e-df2221966dd2"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-chroma langchain-core langchain-experimental langchain-openai pypdf sentence_transformers openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgVCgHZ60nR7"
      },
      "source": [
        "## Import Python Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBbtG-Dq0dxj"
      },
      "source": [
        "Import the required libraries for our lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xrWZ9G9khL8G"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_chroma import Chroma\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-0ExegJ0vI0"
      },
      "source": [
        "Setting the local variables for our lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LLv9vUrshkat"
      },
      "outputs": [],
      "source": [
        "file_dir = 'asp-ai-lab/files'\n",
        "model = \"gpt-3.5-turbo\"\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJK6xuZo0061"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDBUpyUT08dp"
      },
      "source": [
        "### Load the Local Vector Database (Chroma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DoUy2B7ih3FP"
      },
      "outputs": [],
      "source": [
        "def load_vectordb():\n",
        "    # load the documents from the directory\n",
        "    loader = DirectoryLoader(file_dir, use_multithreading=True, loader_cls=PyPDFLoader)\n",
        "    documents = loader.load()\n",
        "    # split the documents into chunks\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "    text_splitter = SemanticChunker(embeddings=embeddings)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    # store the documents and embeddings in the database\n",
        "    db = Chroma.from_documents(docs, embeddings)\n",
        "    return db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65JrzV9U1Jkr"
      },
      "source": [
        "### Query the local vector database (Chroma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hVL9t9BIh7jp"
      },
      "outputs": [],
      "source": [
        "def query_vectordb(query, db):\n",
        "    # query the database\n",
        "    retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5})\n",
        "    docs = retriever.invoke(query)\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwSSgCX91P_G"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPxpkNt21TSB"
      },
      "source": [
        "### Create the Prompt for the User Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0kNp_1vtiBsI"
      },
      "outputs": [],
      "source": [
        "def create_prompt(query, context):\n",
        "    prompt = f\"\"\"\n",
        "    Use the following pieces of context to answer the question at the end.\n",
        "    If you do not know the answer, please think rationally and answer from your own knowledge base.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHY3_Zvr1Y9W"
      },
      "source": [
        "### OpenAI Chat Completions API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-1YW-TCUiHTx"
      },
      "outputs": [],
      "source": [
        "def chat_openai (user_question, client, model):\n",
        "    \"\"\"\n",
        "    This function sends a chat message to the OpenAI API and returns the content of the response.\n",
        "    It takes two parameters: the chat prompt and the model to use for the chat.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\"\n",
        "    {user_question}\n",
        "\n",
        "    Analyze the user's question and provide an answer based upon the context of the question.\n",
        "    \"\"\".format(\n",
        "        user_question=user_question\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful network engineering expert:\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQSAVScR1rsA"
      },
      "source": [
        "## Run the application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUeTWpwxiSx3"
      },
      "outputs": [],
      "source": [
        "db = load_vectordb()\n",
        "query = 'How many 800G interfaces does the PTX10002-36QDD have?'\n",
        "docs = query_vectordb(query, db)\n",
        "# print the results\n",
        "query_result = (docs[0].page_content)\n",
        "prompt = create_prompt(query, query_result)\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "response = chat_openai(prompt, client, model)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqXI-a0AhRwp"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOQMJMcEO+Z5in+NvvLsnxk",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
