{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfierbaugh/asp-ai-lab/blob/main/asp_ai_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6doi3Bd460C"
      },
      "source": [
        "# ASP Lab for AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ASP Lab for AI consists of 3 different labs. \n",
        "\n",
        "* Lab 1 - OpenAI Introductory Lab\n",
        "* Lab 2 - Local VectorDB Lab\n",
        "* Lab 3 - Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feQ7-isy8pL_"
      },
      "source": [
        "## Pre-Lab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkT16Fgi8yaZ"
      },
      "source": [
        "### Step 1: Clone the github repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRJwtHnR5NEJ"
      },
      "source": [
        "Clone the gitbub repo that contains all of the files we will use during the course of this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVka27Nb5Nvu",
        "outputId": "a959cf77-a7e8-4468-a0ba-a26b42fa4052"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mfierbaugh/asp-ai-lab.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWDFK9GH8882"
      },
      "source": [
        "### Step 2: Enter the OpenAI API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJKVg0Oz5Zm1"
      },
      "source": [
        "Enter the OpenAI API Key.  This will be provided to you on Webex Teams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ekDHyH9EeG"
      },
      "source": [
        "### Step 3: Install the required Python models for this lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMBmQ6HT9Oa0",
        "outputId": "7c34221c-74ee-404e-f45b-18c2f752924d"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-chroma langchain-core langchain-experimental langchain-openai pypdf sentence_transformers openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJdhHsY54n4T"
      },
      "source": [
        "## Lab 1: OpenAI Introductory Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edpcQ9GW4mLZ"
      },
      "source": [
        "This lab is a walk through of how to build your first chatbot using openAI.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpnNTAUZ6m2V"
      },
      "source": [
        "### Import Python Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIezl-o84mLa"
      },
      "source": [
        "This lab uses openAI's Chat Completion API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs3AdcTK4mLa"
      },
      "source": [
        "Now that we have the openAI libraries installed, we are going to import those libraies into our code so we can use them. We are also going to add the google.colab library so that we can retrieve our openai API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1u5sP0f4mLa"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti0vjqjH4mLa"
      },
      "source": [
        "We have the ability to use different models in openAI's API.  Let's create a simple variable and store the model name as a string.  We will pass this to openAI via a function later.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kyywo32E4mLa"
      },
      "outputs": [],
      "source": [
        "model = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwV2ZAYW4mLa"
      },
      "source": [
        "This is a function in python that will have 3 things passed to it when we call it.  \n",
        "\n",
        "1. The user's question.\n",
        "2. The client - in our case OpenAI with the API key defined.\n",
        "3. The model we want to use.\n",
        "\n",
        "Coding up the function in this way alows for us to change the model or client if we so desire.  \n",
        "\n",
        "This is using OpenAI's Chat Completions API and contains the inputs along with the instructions for providing an output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gwHbuqg6vF7"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are only using a single function in this lab.  It is all we will need.  \n",
        "\n",
        "The entire prompt is assembled here, including the user's question and anything we combine with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwyxPrMX4mLb"
      },
      "outputs": [],
      "source": [
        "def chat_openai (user_question, client, model):\n",
        "    \"\"\"\n",
        "    This function sends a chat message to the OpenAI API and returns the content of the response.\n",
        "    It takes two parameters: the chat prompt and the model to use for the chat.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\"\n",
        "    {user_question}\n",
        "\n",
        "    Analyze the user's question and provide an answer based upon the context of the question.\n",
        "    \"\"\".format(\n",
        "        user_question=user_question\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a rude cynical assistant network engineer:\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmxLK5rc4mLb"
      },
      "source": [
        "The Chat Completions API supports text and image inputs, and can output text content (including code and JSON).\n",
        "\n",
        "Let's take a closer look at the prompts.\n",
        "\n",
        "The prompt includes the user question and a set of instructions designed to control the output of the response.  \n",
        "\n",
        "Each message object has a role (either system, user, or assistant) and content.\n",
        "\n",
        "*   The system message is optional and can be used to set the behavior of the assistant\n",
        "*   The user messages provide requests or comments for the assistant to respond to\n",
        "*   Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkDPhDJW4mLb"
      },
      "source": [
        "Notice above how we have set the behavior of the assistant by telling it \"You are a rude cynical assistant network engineer:\"\n",
        "\n",
        "We are just having some fun here.  We can make the responses come back snarky, just to show how we can change the behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfS1xyii4mLb"
      },
      "source": [
        "Now, let's create a the user's question.  You can change this if you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhHO6Lfr4mLb"
      },
      "outputs": [],
      "source": [
        "user_question = \"Which routers are better, Juniper or Cisco?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8aFNPrU4mLb"
      },
      "source": [
        "Now, We are going to send the system prompt and the user question to openAI's API using the function we defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM-7sK1b4mLb",
        "outputId": "62892873-9f9e-416d-b5a3-0cdca17ab3c7"
      },
      "outputs": [],
      "source": [
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "response = chat_openai(user_question, client, model)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxbcNVxE64IU"
      },
      "source": [
        "üèÜ Congratulations!  Lab 1 is complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnc-ILjj4mLb"
      },
      "source": [
        "## Lab 2: Local VectorDB Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo6zkQ4K4mLb"
      },
      "source": [
        "This lab will demonstrate how we can create a local vector database and store unstructured data.  We will then use a semantic search of the data that is stored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we will import all of libraries that we will need for this part of the lab. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6y5Xlk74mLc"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_chroma import Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw2XPELp4mLc"
      },
      "source": [
        "We are going to take some local documents that we pulled from the github repository for this lab.  These are PDF files and are now stored in your local runtime enviornment. We will set the \"file_dir\" variable to a string that represents the relative path to the directory where these are stored.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsEO3IN34mLc"
      },
      "outputs": [],
      "source": [
        "file_dir = 'asp-ai-lab/files'\n",
        "embedding_model = 'all-MiniLM-L6-v2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNAYC_gJ4mLd"
      },
      "source": [
        "The Langchain DirectoryLoader let's us load all of the files in the specified file directory.  We will use the variable defined above and pass that to the DirectoryLoader. \n",
        "\n",
        "We then load the docs in that directory with DirectoryLoader's load() function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WF2D-h94mLd"
      },
      "outputs": [],
      "source": [
        "loader = DirectoryLoader(file_dir, use_multithreading=True, loader_cls=PyPDFLoader)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm6UwcaG4mLd"
      },
      "source": [
        "Now that we have the documents loaded, want to split the text from the document into chunks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0P-ChIG4mLd"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=5)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtKKIZ104mLd"
      },
      "source": [
        "We will use OpenAI's embeddings model.  Embeddings are a vector list of floating point numbers. They allow us to measure how things are related by giving them a floating point number and measuring the distance between them.  \n",
        "\n",
        "Embeddings are commonly used for:\n",
        "* Search (where results are ranked by relevance to a query string)\n",
        "* Clustering (where text strings are grouped by similarity)\n",
        "* Recommendations (where items with related text strings are recommended)\n",
        "* Anomaly detection (where outliers with little relatedness are identified)\n",
        "* Diversity measurement (where similarity distributions are analyzed)\n",
        "* Classification (where text strings are classified by their most similar label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0Vjf78h4mLd"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc1JTJQa4mLd"
      },
      "source": [
        "Chroma is an AI-native open-source vector database. After splitting the docs into chunks, we will use the OpenAI embeddings and insert them into a collection. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqfl15AK4mLd"
      },
      "outputs": [],
      "source": [
        "db = Chroma.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ta452lx4mLd"
      },
      "source": [
        "The query variable contains the question we will ask on the docs.  doc_search runs the similarity search on the Chroma vector database. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDXpkRQi4mLd"
      },
      "outputs": [],
      "source": [
        "query = 'How many 400 GigE interfaces can the PTX10002-36QDD support?'\n",
        "doc_search = db.similarity_search(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeBStoFF4mLe"
      },
      "source": [
        "Printing the result, we should see some chunks that we think are the most similar to the question. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1AmwpUk4mLe"
      },
      "outputs": [],
      "source": [
        "print (doc_search[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üèÜ Congratulations!  Lab 2 is complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZC3JMe34mLe"
      },
      "source": [
        "## Lab 3: OpenAI LAB - Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkvWivXp4mLf"
      },
      "source": [
        "This lab combines the previous two labs and will show how we can retrieve information locally and augment the input.  Then we will specifically instruct the OpenAI Chat Completions API (prompt) to answer the question based upon the provided data.  This is an example of RAG (Retrieval Augmented Generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwWc-2j44mLf"
      },
      "source": [
        "### Import Python Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6TuRI9P4mLf"
      },
      "source": [
        "Import the required libraries for our lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttTsVX3T4mLf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_chroma import Chroma\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UbUz0mb4mLf"
      },
      "source": [
        "### Load the Local Vector Database (Chroma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the same as we did in lab 2 execpt we are loading the docs, doing the embeddings, splitting, and inserting into the vectordb all in a single function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Rkp4TYU4mLf"
      },
      "outputs": [],
      "source": [
        "def load_vectordb():\n",
        "    # load the documents from the directory\n",
        "    loader = DirectoryLoader(file_dir, use_multithreading=True, loader_cls=PyPDFLoader)\n",
        "    documents = loader.load()\n",
        "    # split the documents into chunks\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "    text_splitter = SemanticChunker(embeddings=embeddings)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    # store the documents and embeddings in the database\n",
        "    db = Chroma.from_documents(docs, embeddings)\n",
        "    return db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aExdXr8y4mLf"
      },
      "source": [
        "### Query the local vector database (Chroma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a python function to query our vector database. When we call this, we will pass it the db and the query string.  \n",
        "\n",
        "What we are doing differently this time is that we are using a similarity search with scores.  Since it measures the distance (similarity), the lower the number (shorter the distance), the more similar it is.\n",
        "\n",
        "By default Chroma returns 4 documents, but we restrict it to return only documents that meet a relevance score threshold. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY4sO-Ge4mLf"
      },
      "outputs": [],
      "source": [
        "def query_vectordb(query, db):\n",
        "    # query the database\n",
        "    retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5})\n",
        "    docs = retriever.invoke(query)\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW_1tVYr4mLf"
      },
      "source": [
        "### Create the Prompt for the User Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You will see that we are going to combine the user question with the retrieved data. The context will be the information we retrieved locally from the vector database.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrbQam2p4mLg"
      },
      "outputs": [],
      "source": [
        "def create_prompt(query, context):\n",
        "    prompt = f\"\"\"\n",
        "    Use the following pieces of context to answer the question at the end.\n",
        "    If you do not know the answer, please think rationally and answer from your own knowledge base.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZlypPuX4mLg"
      },
      "source": [
        "### OpenAI Chat Completions API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is very similar to what we used in lab 1.  Now we are a helpful network engineer rather than a rude cynical one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3d_tQHg4mLg"
      },
      "outputs": [],
      "source": [
        "def chat_openai (user_question, client, model):\n",
        "    \"\"\"\n",
        "    This function sends a chat message to the OpenAI API and returns the content of the response.\n",
        "    It takes two parameters: the chat prompt and the model to use for the chat.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\"\n",
        "    {user_question}\n",
        "\n",
        "    Analyze the user's question and provide an answer based upon the context of the question.\n",
        "    \"\"\".format(\n",
        "        user_question=user_question\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful network engineering expert:\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlE_-DSm4mLg"
      },
      "source": [
        "### Run the application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's just ask OpenAI to give us some information about the device.  Remember, given that the training data stopped before this network device was available or that there was any information about it, gpt-3.5-turbo should have no idea what this actually is. \n",
        "\n",
        "Note - what happened? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_nodb = 'How many 400 GigE interfaces can the PTX10002-36QDD support?'\n",
        "response = chat_openai(question_nodb, client, model)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenAI likely tried to answer the question and made things up.  This is what is known as a hallucination. \n",
        "\n",
        "Now we are going to show the difference.  \n",
        "\n",
        "We have a foundation model (gpt-3.5-turbo) - like an untrained employee. \n",
        "We are going to load the vector database with the pdfs after they have been chunked and embeddings done.  \n",
        "We are going to do a cosine similarity search, with a threshold. \n",
        "We are going to add that to the user's question and send it to the foudnation model.  \n",
        "\n",
        "No fine-tuning. Just the model + the vectorstore data.\n",
        "\n",
        "Behold, the power of RAG..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r6TC8MO4mLg"
      },
      "outputs": [],
      "source": [
        "db = load_vectordb()\n",
        "query = 'How many 400 GigE interfaces can the PTX10002-36QDD support?'\n",
        "docs = query_vectordb(query, db)\n",
        "# print the results\n",
        "query_result = (docs[0].page_content)\n",
        "prompt = create_prompt(query, query_result)\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "response = chat_openai(prompt, client, model)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXxIe27u4mLg"
      },
      "source": [
        "üèÜ Congratulations!  Lab 3 is complete"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
