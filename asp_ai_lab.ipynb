{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfierbaugh/asp-ai-lab/blob/main/asp_ai_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6doi3Bd460C"
      },
      "source": [
        "# ASP Lab for AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ASP Lab for AI consists of 3 different labs. \n",
        "\n",
        "**Lab 1 - OpenAI Introductory Lab**  \n",
        "\n",
        "In this lab you will build your first chatbot using openAI and explore how changing the prompt impacts the output. \n",
        "\n",
        "**Lab 2 - Local VectorDB Lab**\n",
        "\n",
        "In this lab you will take local PDF files, break them into chunks, assign embeddings (vectors), and store in a local vector database.  You will then ask a question and use only the database to return an answer.  No LLM will be used for this lab. \n",
        "\n",
        "**Lab 3 - Retrieval Augmented Generation**\n",
        "\n",
        "This lab will combine the previous 2 labs to show how to retrieve data from a local vector database and augment the prompt to the LLM.  Hint: The LLM doesn't know the answer without the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feQ7-isy8pL_"
      },
      "source": [
        "## Pre-Lab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkT16Fgi8yaZ"
      },
      "source": [
        "### Step 1: Clone the github repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRJwtHnR5NEJ"
      },
      "source": [
        "Clone the gitbub repo that contains all of the files we will use during the course of this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVka27Nb5Nvu",
        "outputId": "a959cf77-a7e8-4468-a0ba-a26b42fa4052"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mfierbaugh/asp-ai-lab.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWDFK9GH8882"
      },
      "source": [
        "### Step 2: Enter the OpenAI API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJKVg0Oz5Zm1"
      },
      "source": [
        "Enter the OpenAI API Key.  This will be provided to you on Webex Teams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ekDHyH9EeG"
      },
      "source": [
        "### Step 3: Install the required Python models for this lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMBmQ6HT9Oa0",
        "outputId": "7c34221c-74ee-404e-f45b-18c2f752924d"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-chroma langchain-core langchain-experimental langchain-openai pypdf sentence_transformers openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJdhHsY54n4T"
      },
      "source": [
        "## Lab 1: OpenAI Introductory Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edpcQ9GW4mLZ"
      },
      "source": [
        "This lab is a walk through of how to build your first chatbot using openAI.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpnNTAUZ6m2V"
      },
      "source": [
        "### Import Python Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIezl-o84mLa"
      },
      "source": [
        "This lab uses openAI's Chat Completion API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs3AdcTK4mLa"
      },
      "source": [
        "Now that we have the openAI libraries installed, we are going to import those libraies into our code so we can use them. We are also going to add the google.colab library so that we can retrieve our openai API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1u5sP0f4mLa"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti0vjqjH4mLa"
      },
      "source": [
        "We have the ability to use different models in openAI's API.  Let's create a simple variable and store the model name as a string.  We will pass this to openAI via a function later.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kyywo32E4mLa"
      },
      "outputs": [],
      "source": [
        "model = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwV2ZAYW4mLa"
      },
      "source": [
        "This is a function in python that will have 3 things passed to it when we call it.  \n",
        "\n",
        "1. The user's question.\n",
        "2. The client - in our case OpenAI with the API key defined.\n",
        "3. The model we want to use.\n",
        "\n",
        "Coding up the function in this way alows for us to change the model or client if we so desire.  \n",
        "\n",
        "This is using OpenAI's Chat Completions API and contains the inputs along with the instructions for providing an output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gwHbuqg6vF7"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are only using a single function in this lab.  It is all we will need.  \n",
        "\n",
        "The entire prompt is assembled here, including the user's question and anything we combine with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwyxPrMX4mLb"
      },
      "outputs": [],
      "source": [
        "def chat_openai (user_question, client, model):\n",
        "    \"\"\"\n",
        "    This function sends a chat message to the OpenAI API and returns the content of the response.\n",
        "    It takes two parameters: the chat prompt and the model to use for the chat.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\"\n",
        "    {user_question}\n",
        "\n",
        "    Analyze the user's question and provide an answer based upon the context of the question.\n",
        "    \"\"\".format(\n",
        "        user_question=user_question\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a rude cynical assistant network engineer:\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmxLK5rc4mLb"
      },
      "source": [
        "The Chat Completions API supports text and image inputs, and can output text content (including code and JSON).\n",
        "\n",
        "Let's take a closer look at the prompts.\n",
        "\n",
        "The prompt includes the user question and a set of instructions designed to control the output of the response.  \n",
        "\n",
        "Each message object has a role (either system, user, or assistant) and content.\n",
        "\n",
        "*   The system message is optional and can be used to set the behavior of the assistant\n",
        "*   The user messages provide requests or comments for the assistant to respond to\n",
        "*   Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkDPhDJW4mLb"
      },
      "source": [
        "Notice above how we have set the behavior of the assistant by telling it \"You are a rude cynical assistant network engineer:\"\n",
        "\n",
        "We are just having some fun here.  We can make the responses come back snarky, just to show how we can change the behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfS1xyii4mLb"
      },
      "source": [
        "Now, let's create a the user's question.  You can change this if you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhHO6Lfr4mLb"
      },
      "outputs": [],
      "source": [
        "user_question = \"Which routers are better, Juniper or Cisco?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8aFNPrU4mLb"
      },
      "source": [
        "Now, We are going to send the system prompt and the user question to openAI's API using the function we defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM-7sK1b4mLb",
        "outputId": "62892873-9f9e-416d-b5a3-0cdca17ab3c7"
      },
      "outputs": [],
      "source": [
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "response = chat_openai(user_question, client, model)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxbcNVxE64IU"
      },
      "source": [
        "üèÜ Congratulations!  Lab 1 is complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnc-ILjj4mLb"
      },
      "source": [
        "## Lab 2: Local VectorDB Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo6zkQ4K4mLb"
      },
      "source": [
        "This lab will demonstrate how we can create a local vector database and store unstructured data.  We will then use a semantic search of the data that is stored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we will import all of libraries that we will need for this part of the lab. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6y5Xlk74mLc"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_chroma import Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw2XPELp4mLc"
      },
      "source": [
        "We are going to take some local documents that we pulled from the github repository for this lab.  These are PDF files and are now stored in your local runtime enviornment. We will set the \"file_dir\" variable to a string that represents the relative path to the directory where these are stored.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsEO3IN34mLc"
      },
      "outputs": [],
      "source": [
        "file_dir = 'asp-ai-lab/files'\n",
        "embedding_model = 'all-MiniLM-L6-v2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNAYC_gJ4mLd"
      },
      "source": [
        "The Langchain DirectoryLoader let's us load all of the files in the specified file directory.  We will use the variable defined above and pass that to the DirectoryLoader. \n",
        "\n",
        "We then load the docs in that directory with DirectoryLoader's load() function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WF2D-h94mLd"
      },
      "outputs": [],
      "source": [
        "loader = DirectoryLoader(file_dir, use_multithreading=True, loader_cls=PyPDFLoader)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm6UwcaG4mLd"
      },
      "source": [
        "Now that we have the documents loaded, want to split the text from the document into chunks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0P-ChIG4mLd"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=5)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtKKIZ104mLd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0Vjf78h4mLd"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc1JTJQa4mLd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqfl15AK4mLd"
      },
      "outputs": [],
      "source": [
        "db = Chroma.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ta452lx4mLd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDXpkRQi4mLd"
      },
      "outputs": [],
      "source": [
        "query = 'How many 400 GigE interfaces can the PTX10002-36QDD support?'\n",
        "docs = db.similarity_search(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeBStoFF4mLe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1AmwpUk4mLe"
      },
      "outputs": [],
      "source": [
        "print (docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üèÜ Congratulations!  Lab 2 is complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZC3JMe34mLe"
      },
      "source": [
        "## Lab 3: OpenAI LAB - Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkvWivXp4mLf"
      },
      "source": [
        "This lab combines the previous two labs and will show how we can retrieve information locally and augment the input.  Then we will specifically instruct the OpenAI Chat Completions API (prompt) to answer the question based upon the provided data.  This is an example of RAG (Retrieval Augmented Generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwWc-2j44mLf"
      },
      "source": [
        "### Import Python Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6TuRI9P4mLf"
      },
      "source": [
        "Import the required libraries for our lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttTsVX3T4mLf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_chroma import Chroma\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItENGTOy4mLf"
      },
      "source": [
        "Setting the local variables for our lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i-AA5144mLf"
      },
      "outputs": [],
      "source": [
        "file_dir = 'asp-ai-lab/files'\n",
        "model = \"gpt-3.5-turbo\"\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UbUz0mb4mLf"
      },
      "source": [
        "### Load the Local Vector Database (Chroma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Rkp4TYU4mLf"
      },
      "outputs": [],
      "source": [
        "def load_vectordb():\n",
        "    # load the documents from the directory\n",
        "    loader = DirectoryLoader(file_dir, use_multithreading=True, loader_cls=PyPDFLoader)\n",
        "    documents = loader.load()\n",
        "    # split the documents into chunks\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "    text_splitter = SemanticChunker(embeddings=embeddings)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    # store the documents and embeddings in the database\n",
        "    db = Chroma.from_documents(docs, embeddings)\n",
        "    return db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aExdXr8y4mLf"
      },
      "source": [
        "### Query the local vector database (Chroma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY4sO-Ge4mLf"
      },
      "outputs": [],
      "source": [
        "def query_vectordb(query, db):\n",
        "    # query the database\n",
        "    retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5})\n",
        "    docs = retriever.invoke(query)\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiPxfE784mLf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW_1tVYr4mLf"
      },
      "source": [
        "### Create the Prompt for the User Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrbQam2p4mLg"
      },
      "outputs": [],
      "source": [
        "def create_prompt(query, context):\n",
        "    prompt = f\"\"\"\n",
        "    Use the following pieces of context to answer the question at the end.\n",
        "    If you do not know the answer, please think rationally and answer from your own knowledge base.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZlypPuX4mLg"
      },
      "source": [
        "### OpenAI Chat Completions API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3d_tQHg4mLg"
      },
      "outputs": [],
      "source": [
        "def chat_openai (user_question, client, model):\n",
        "    \"\"\"\n",
        "    This function sends a chat message to the OpenAI API and returns the content of the response.\n",
        "    It takes two parameters: the chat prompt and the model to use for the chat.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\"\n",
        "    {user_question}\n",
        "\n",
        "    Analyze the user's question and provide an answer based upon the context of the question.\n",
        "    \"\"\".format(\n",
        "        user_question=user_question\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful network engineering expert:\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlE_-DSm4mLg"
      },
      "source": [
        "### Run the application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_nodb = 'How many 400 GigE interfaces can the PTX10002-36QDD support?'\n",
        "response = chat_openai(question_nodb, client, model)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to show the difference.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r6TC8MO4mLg"
      },
      "outputs": [],
      "source": [
        "db = load_vectordb()\n",
        "query = 'How many 400 GigE interfaces can the PTX10002-36QDD support?'\n",
        "docs = query_vectordb(query, db)\n",
        "# print the results\n",
        "query_result = (docs[0].page_content)\n",
        "prompt = create_prompt(query, query_result)\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "response = chat_openai(prompt, client, model)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXxIe27u4mLg"
      },
      "source": [
        "üèÜ Congratulations!  Lab 3 is complete"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
